{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"project_3_starter.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"uLYbBqwMxS4B","colab_type":"text"},"source":["# Project 3: Smart Beta Portfolio and Portfolio Optimization\n","\n","## Overview\n","\n","\n","Smart beta has a broad meaning, but we can say in practice that when we use the universe of stocks from an index, and then apply some weighting scheme other than market cap weighting, it can be considered a type of smart beta fund.  A Smart Beta portfolio generally gives investors exposure or \"beta\" to one or more types of market characteristics (or factors) that are believed to predict prices while giving investors a diversified broad exposure to a particular market. Smart Beta portfolios generally target momentum, earnings quality, low volatility, and dividends or some combination. Smart Beta Portfolios are generally rebalanced infrequently and follow relatively simple rules or algorithms that are passively managed.  Model changes to these types of funds are also rare requiring prospectus filings with US Security and Exchange Commission in the case of US focused mutual funds or ETFs.. Smart Beta portfolios are generally long-only, they do not short stocks.\n","\n","In contrast, a purely alpha-focused quantitative fund may use multiple models or algorithms to create a portfolio. The portfolio manager retains discretion in upgrading or changing the types of models and how often to rebalance the portfolio in attempt to maximize performance in comparison to a stock benchmark.  Managers may have discretion to short stocks in portfolios.\n","\n","Imagine you're a portfolio manager, and wish to try out some different portfolio weighting methods.\n","\n","One way to design portfolio is to look at certain accounting measures (fundamentals) that, based on past trends, indicate stocks that produce better results.  \n","\n","\n","For instance, you may start with a hypothesis that dividend-issuing stocks tend to perform better than stocks that do not. This may not always be true of all companies; for instance, Apple does not issue dividends, but has had good historical performance.  The hypothesis about dividend-paying stocks may go something like this: \n","\n","Companies that regularly issue dividends may also be more prudent in allocating their available cash, and may indicate that they are more conscious of prioritizing shareholder interests.  For example, a CEO may decide to reinvest cash into pet projects that produce low returns.  Or, the CEO may do some analysis, identify that reinvesting within the company produces lower returns compared to a diversified portfolio, and so decide that shareholders would be better served if they were given the cash (in the form of dividends).  So according to this hypothesis, dividends may be both a proxy for how the company is doing (in terms of earnings and cash flow), but also a signal that the company acts in the best interest of its shareholders.  Of course, it's important to test whether this works in practice.\n","\n","\n","You may also have another hypothesis, with which you wish to design a portfolio that can then be made into an ETF.  You may find that investors may wish to invest in passive beta funds, but wish to have less risk exposure (less volatility) in their investments.  The goal of having a low volatility fund that still produces returns similar to an index may be appealing to investors who have a shorter investment time horizon, and so are more risk averse.\n","\n","So the objective of your proposed portfolio is to design a portfolio that closely tracks an index, while also minimizing the portfolio variance.  Also, if this portfolio can match the returns of the index with less volatility, then it has a higher risk-adjusted return (same return, lower volatility).\n","\n","Smart Beta ETFs can be designed with both of these two general methods (among others): alternative weighting and minimum volatility ETF.\n","\n","\n","## Instructions\n","Each problem consists of a function to implement and instructions on how to implement the function.  The parts of the function that need to be implemented are marked with a `# TODO` comment. After implementing the function, run the cell to test it against the unit tests we've provided. For each problem, we provide one or more unit tests from our `project_tests` package. These unit tests won't tell you if your answer is correct, but will warn you of any major errors. Your code will be checked for the correct solution when you submit it to Udacity.\n","\n","## Packages\n","When you implement the functions, you'll only need to you use the packages you've used in the classroom, like [Pandas](https://pandas.pydata.org/) and [Numpy](http://www.numpy.org/). These packages will be imported for you. We recommend you don't add any import statements, otherwise the grader might not be able to run your code.\n","\n","The other packages that we're importing are `helper`, `project_helper`, and `project_tests`. These are custom packages built to help you solve the problems.  The `helper` and `project_helper` module contains utility functions and graph functions. The `project_tests` contains the unit tests for all the problems.\n","### Install Packages"]},{"cell_type":"code","metadata":{"id":"5uRp9QyjxS4C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594486875640,"user_tz":300,"elapsed":85590,"user":{"displayName":"Mad Alan","photoUrl":"","userId":"18191286341388010000"}},"outputId":"35962fbf-2cb4-48d7-d1ef-cf98230fb7eb"},"source":["\n","\n","import sys\n","!{sys.executable} -m pip install -r requirements.txt"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting colour==0.1.5\n","  Downloading https://files.pythonhosted.org/packages/74/46/e81907704ab203206769dee1385dc77e1407576ff8f50a0681d0a6b541be/colour-0.1.5-py2.py3-none-any.whl\n","Collecting cvxpy==1.0.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/59/2613468ffbbe3a818934d06b81b9f4877fe054afbf4f99d2f43f398a0b34/cvxpy-1.0.3.tar.gz (880kB)\n","\u001b[K     |████████████████████████████████| 880kB 5.3MB/s \n","\u001b[?25hRequirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.10.0)\n","Collecting numpy==1.14.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/1e/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n","\u001b[K     |████████████████████████████████| 12.2MB 14.2MB/s \n","\u001b[?25hCollecting pandas==0.21.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/e1/6c514df670b887c77838ab856f57783c07e8760f2e3d5939203a39735e0e/pandas-0.21.1-cp36-cp36m-manylinux1_x86_64.whl (26.2MB)\n","\u001b[K     |████████████████████████████████| 26.2MB 1.3MB/s \n","\u001b[?25hCollecting plotly==2.2.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/a6/8214b6564bf4ace9bec8a26e7f89832792be582c042c47c912d3201328a0/plotly-2.2.3.tar.gz (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 42.7MB/s \n","\u001b[?25hCollecting pyparsing==2.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/8a/718fd7d3458f9fab8e67186b00abdd345b639976bc7fb3ae722e1b026a50/pyparsing-2.2.0-py2.py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n","\u001b[?25hCollecting python-dateutil==2.6.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/0d/7ed381ab4fe80b8ebf34411d14f253e1cf3e56e2820ffa1d8844b23859a2/python_dateutil-2.6.1-py2.py3-none-any.whl (194kB)\n","\u001b[K     |████████████████████████████████| 194kB 49.1MB/s \n","\u001b[?25hCollecting pytz==2017.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/7f/e7d1acbd433b929168a4fb4182a2ff3c33653717195a26c1de099ad1ef29/pytz-2017.3-py2.py3-none-any.whl (511kB)\n","\u001b[K     |████████████████████████████████| 512kB 46.3MB/s \n","\u001b[?25hCollecting requests==2.18.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/df/50aa1999ab9bde74656c2919d9c0c085fd2b3775fd3eca826012bef76d8c/requests-2.18.4-py2.py3-none-any.whl (88kB)\n","\u001b[K     |████████████████████████████████| 92kB 10.3MB/s \n","\u001b[?25hCollecting scipy==1.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/5e/caa01ba7be11600b6a9d39265440d7b3be3d69206da887c42bef049521f2/scipy-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (50.0MB)\n","\u001b[K     |████████████████████████████████| 50.0MB 72kB/s \n","\u001b[?25hCollecting scikit-learn==0.19.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/2d/9fbc7baa5f44bc9e88ffb7ed32721b879bfa416573e85031e16f52569bc9/scikit_learn-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n","\u001b[K     |████████████████████████████████| 12.4MB 46.3MB/s \n","\u001b[?25hCollecting six==1.11.0\n","  Downloading https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\n","Collecting tqdm==4.19.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/3c/341b4fa23cb3abc335207dba057c790f3bb329f6757e1fcd5d347bcf8308/tqdm-4.19.5-py2.py3-none-any.whl (51kB)\n","\u001b[K     |████████████████████████████████| 61kB 9.3MB/s \n","\u001b[?25hRequirement already satisfied: osqp in /usr/local/lib/python3.6/dist-packages (from cvxpy==1.0.3->-r requirements.txt (line 2)) (0.6.1)\n","Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.6/dist-packages (from cvxpy==1.0.3->-r requirements.txt (line 2)) (2.0.7.post1)\n","Requirement already satisfied: scs>=1.1.3 in /usr/local/lib/python3.6/dist-packages (from cvxpy==1.0.3->-r requirements.txt (line 2)) (2.1.2)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from cvxpy==1.0.3->-r requirements.txt (line 2)) (0.70.10)\n","Collecting fastcache\n","  Downloading https://files.pythonhosted.org/packages/5f/a3/b280cba4b4abfe5f5bdc643e6c9d81bf3b9dc2148a11e5df06b6ba85a560/fastcache-1.1.0.tar.gz\n","Requirement already satisfied: toolz in /usr/local/lib/python3.6/dist-packages (from cvxpy==1.0.3->-r requirements.txt (line 2)) (0.10.0)\n","Requirement already satisfied: decorator>=4.0.6 in /usr/local/lib/python3.6/dist-packages (from plotly==2.2.3->-r requirements.txt (line 6)) (4.4.2)\n","Requirement already satisfied: nbformat>=4.2 in /usr/local/lib/python3.6/dist-packages (from plotly==2.2.3->-r requirements.txt (line 6)) (5.0.7)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.18.4->-r requirements.txt (line 10)) (3.0.4)\n","Collecting idna<2.7,>=2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/cc/6dd9a3869f15c2edfab863b992838277279ce92663d334df9ecf5106f5c6/idna-2.6-py2.py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 9.2MB/s \n","\u001b[?25hCollecting urllib3<1.23,>=1.21.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/cb/6965947c13a94236f6d4b8223e21beb4d576dc72e8130bd7880f600839b8/urllib3-1.22-py2.py3-none-any.whl (132kB)\n","\u001b[K     |████████████████████████████████| 133kB 51.8MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.18.4->-r requirements.txt (line 10)) (2020.6.20)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from osqp->cvxpy==1.0.3->-r requirements.txt (line 2)) (0.16.0)\n","Requirement already satisfied: dill>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from multiprocess->cvxpy==1.0.3->-r requirements.txt (line 2)) (0.3.2)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 6)) (4.6.3)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 6)) (2.6.0)\n","Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 6)) (4.3.3)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 6)) (0.2.0)\n","Building wheels for collected packages: cvxpy, plotly, fastcache\n","  Building wheel for cvxpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cvxpy: filename=cvxpy-1.0.3-cp36-cp36m-linux_x86_64.whl size=2021603 sha256=fcdefecbac4a081983a2a320e4b8b897843829a54e116685b1f9e19ea5f945ba\n","  Stored in directory: /root/.cache/pip/wheels/2b/60/0b/0c2596528665e21d698d6f84a3406c52044c7b4ca6ac737cf3\n","  Building wheel for plotly (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for plotly: filename=plotly-2.2.3-cp36-none-any.whl size=1122402 sha256=dc47bb562fe1c2d6dd6b63200535840ac8775a0ca6283406edda5a12cc5c5565\n","  Stored in directory: /root/.cache/pip/wheels/98/54/81/dd92d5b0858fac680cd7bdb8800eb26c001dd9f5dc8b1bc0ba\n","  Building wheel for fastcache (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fastcache: filename=fastcache-1.1.0-cp36-cp36m-linux_x86_64.whl size=39208 sha256=7b1c60e6b0f2661a99b7a8526dcd56f8e52785496ebae47ed43c48f08cce6e70\n","  Stored in directory: /root/.cache/pip/wheels/6a/80/bf/30024738b03fa5aa521e2a2ac952a8d77d0c65e68d92bcd3b6\n","Successfully built cvxpy plotly fastcache\n","\u001b[31mERROR: yellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: xarray 0.15.1 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: xarray 0.15.1 has requirement pandas>=0.25, but you'll have pandas 0.21.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: umap-learn 0.4.4 has requirement numpy>=1.17, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: umap-learn 0.4.4 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: umap-learn 0.4.4 has requirement scipy>=1.3.1, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tifffile 2020.6.3 has requirement numpy>=1.15.1, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.2.0 has requirement numpy<2.0,>=1.16.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.2.0 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.2.0 has requirement six>=1.12.0, but you'll have six 1.11.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow-hub 0.8.0 has requirement six>=1.12.0, but you'll have six 1.11.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow-datasets 2.1.0 has requirement requests>=2.19.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorboard 2.2.2 has requirement requests<3,>=2.21.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n","\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.19.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: seaborn 0.10.1 has requirement pandas>=0.22.0, but you'll have pandas 0.21.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: seaborn 0.10.1 has requirement scipy>=1.0.1, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: plotnine 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.21.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: numba 0.48.0 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: mizani 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.21.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: imbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.0.0; python_version >= \"3.0\", but you'll have pandas 0.21.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.11.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: featuretools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.21.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fbprophet 0.6 has requirement pandas>=0.23.4, but you'll have pandas 0.21.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fbprophet 0.6 has requirement python-dateutil>=2.8.0, but you'll have python-dateutil 2.6.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fastai 1.0.61 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fancyimpute 0.4.3 has requirement cvxpy>=1.0.6, but you'll have cvxpy 1.0.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: cufflinks 0.17.3 has requirement plotly>=4.1.1, but you'll have plotly 2.2.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: astropy 4.0.1.post1 has requirement numpy>=1.16, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: colour, fastcache, six, numpy, scipy, cvxpy, pytz, python-dateutil, pandas, idna, urllib3, requests, plotly, pyparsing, scikit-learn, tqdm\n","  Found existing installation: six 1.12.0\n","    Uninstalling six-1.12.0:\n","      Successfully uninstalled six-1.12.0\n","  Found existing installation: numpy 1.18.5\n","    Uninstalling numpy-1.18.5:\n","      Successfully uninstalled numpy-1.18.5\n","  Found existing installation: scipy 1.4.1\n","    Uninstalling scipy-1.4.1:\n","      Successfully uninstalled scipy-1.4.1\n","  Found existing installation: cvxpy 1.0.31\n","    Uninstalling cvxpy-1.0.31:\n","      Successfully uninstalled cvxpy-1.0.31\n","  Found existing installation: pytz 2018.9\n","    Uninstalling pytz-2018.9:\n","      Successfully uninstalled pytz-2018.9\n","  Found existing installation: python-dateutil 2.8.1\n","    Uninstalling python-dateutil-2.8.1:\n","      Successfully uninstalled python-dateutil-2.8.1\n","  Found existing installation: pandas 1.0.5\n","    Uninstalling pandas-1.0.5:\n","      Successfully uninstalled pandas-1.0.5\n","  Found existing installation: idna 2.9\n","    Uninstalling idna-2.9:\n","      Successfully uninstalled idna-2.9\n","  Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Found existing installation: plotly 4.4.1\n","    Uninstalling plotly-4.4.1:\n","      Successfully uninstalled plotly-4.4.1\n","  Found existing installation: pyparsing 2.4.7\n","    Uninstalling pyparsing-2.4.7:\n","      Successfully uninstalled pyparsing-2.4.7\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","  Found existing installation: tqdm 4.41.1\n","    Uninstalling tqdm-4.41.1:\n","      Successfully uninstalled tqdm-4.41.1\n","Successfully installed colour-0.1.5 cvxpy-1.0.3 fastcache-1.1.0 idna-2.6 numpy-1.14.5 pandas-0.21.1 plotly-2.2.3 pyparsing-2.2.0 python-dateutil-2.6.1 pytz-2017.3 requests-2.18.4 scikit-learn-0.19.1 scipy-1.0.0 six-1.11.0 tqdm-4.19.5 urllib3-1.22\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4oWVMAVixS4H","colab_type":"text"},"source":["### Load Packages"]},{"cell_type":"code","metadata":{"id":"REDrN1BHxS4I","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1594486922955,"user_tz":300,"elapsed":888,"user":{"displayName":"Mad Alan","photoUrl":"","userId":"18191286341388010000"}},"outputId":"2408c759-ce84-4ca8-b509-74df48232468"},"source":["import pandas as pd\n","import numpy as np\n","import helper\n","import project_helper\n","import project_tests"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/vnd.plotly.v1+html":"<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>","text/html":["<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"FTjWe7w-xS4L","colab_type":"text"},"source":["## Market Data\n","### Load Data\n","For this universe of stocks, we'll be selecting large dollar volume stocks. We're using this universe, since it is highly liquid."]},{"cell_type":"code","metadata":{"id":"3E3CzYRfxS4L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":540},"executionInfo":{"status":"error","timestamp":1594486927512,"user_tz":300,"elapsed":1332,"user":{"displayName":"Mad Alan","photoUrl":"","userId":"18191286341388010000"}},"outputId":"23ed470e-cb3c-484b-aec3-4dba747587fb"},"source":["df = pd.read_csv('eod-quotemedia.csv')\n","\n","percent_top_dollar = 0.2\n","high_volume_symbols = project_helper.large_dollar_volume_stocks(df, 'adj_close', 'adj_volume', percent_top_dollar)\n","df = df[df['ticker'].isin(high_volume_symbols)]\n","\n","close = df.reset_index().pivot(index='date', columns='ticker', values='adj_close')\n","volume = df.reset_index().pivot(index='date', columns='ticker', values='adj_volume')\n","dividends = df.reset_index().pivot(index='date', columns='ticker', values='dividends')"],"execution_count":6,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-44c02d24aecf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpercent_top_dollar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhigh_volume_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlarge_dollar_volume_stocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'adj_close'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'adj_volume'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercent_top_dollar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ticker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh_volume_symbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/project_helper.py\u001b[0m in \u001b[0;36mlarge_dollar_volume_stocks\u001b[0;34m(df, price_column, volume_column, top_percent)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mof\u001b[0m \u001b[0mlarge\u001b[0m \u001b[0mdollar\u001b[0m \u001b[0mvolume\u001b[0m \u001b[0mstock\u001b[0m \u001b[0msymbols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mdollar_traded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ticker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvolume_column\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprice_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdollar_traded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdollar_traded\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtop_percent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n","\u001b[0;32m/content/project_helper.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mof\u001b[0m \u001b[0mlarge\u001b[0m \u001b[0mdollar\u001b[0m \u001b[0mvolume\u001b[0m \u001b[0mstock\u001b[0m \u001b[0msymbols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mdollar_traded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ticker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvolume_column\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprice_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdollar_traded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdollar_traded\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtop_percent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(left, right)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op, str_rep)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mna_arithmetic_op\u001b[0;34m(left, right, op, str_rep)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_NUMEXPR_INSTALLED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name '_values_from_object'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"9320Slp3xS4O","colab_type":"text"},"source":["### View Data\n","To see what one of these 2-d matrices looks like, let's take a look at the closing prices matrix."]},{"cell_type":"code","metadata":{"id":"Ef8LTFBhxS4P","colab_type":"code","colab":{},"outputId":"37538a85-0ba9-4841-9172-374e308e7a15"},"source":["project_helper.print_dataframe(close)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.plotly.v1+json":{"data":[{"cells":{"fill":{"color":["silver","white"]},"font":{"size":13},"line":{"color":"silver"},"values":[["2013-07-01","2013-07-02","2013-07-03","2013-07-05","2013-07-08","2013-07-09","2013-07-10","2013-07-11","2013-07-12","2013-07-15","..."],["16.176","15.820","16.128","16.215","16.311","16.715","16.532","16.725","16.908","17.100","..."],["53.109","54.312","54.612","54.173","53.866","54.813","54.603","55.454","55.353","55.474","..."],["34.924","35.428","35.445","35.856","36.662","36.360","36.855","37.082","38.157","37.793","..."],["...","...","...","...","...","...","...","...","...","...","..."]]},"columnwidth":[1,3],"header":{"fill":{"color":"silver"},"font":{"size":13},"line":{"color":"silver"},"values":["","AAL","AAPL","ABBV","..."]},"type":"table"}],"layout":{}},"text/html":["<div id=\"5d447108-c145-4497-aefe-4c8ff7bc1ee5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"5d447108-c145-4497-aefe-4c8ff7bc1ee5\", [{\"type\": \"table\", \"columnwidth\": [1, 3], \"header\": {\"values\": [\"\", \"AAL\", \"AAPL\", \"ABBV\", \"...\"], \"line\": {\"color\": \"silver\"}, \"fill\": {\"color\": \"silver\"}, \"font\": {\"size\": 13}}, \"cells\": {\"values\": [[\"2013-07-01\", \"2013-07-02\", \"2013-07-03\", \"2013-07-05\", \"2013-07-08\", \"2013-07-09\", \"2013-07-10\", \"2013-07-11\", \"2013-07-12\", \"2013-07-15\", \"...\"], [\"16.176\", \"15.820\", \"16.128\", \"16.215\", \"16.311\", \"16.715\", \"16.532\", \"16.725\", \"16.908\", \"17.100\", \"...\"], [\"53.109\", \"54.312\", \"54.612\", \"54.173\", \"53.866\", \"54.813\", \"54.603\", \"55.454\", \"55.353\", \"55.474\", \"...\"], [\"34.924\", \"35.428\", \"35.445\", \"35.856\", \"36.662\", \"36.360\", \"36.855\", \"37.082\", \"38.157\", \"37.793\", \"...\"], [\"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\"]], \"line\": {\"color\": \"silver\"}, \"fill\": {\"color\": [\"silver\", \"white\"]}, \"font\": {\"size\": 13}}}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"displayModeBar\": false})});</script>"],"text/vnd.plotly.v1+html":"<div id=\"5d447108-c145-4497-aefe-4c8ff7bc1ee5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"5d447108-c145-4497-aefe-4c8ff7bc1ee5\", [{\"type\": \"table\", \"columnwidth\": [1, 3], \"header\": {\"values\": [\"\", \"AAL\", \"AAPL\", \"ABBV\", \"...\"], \"line\": {\"color\": \"silver\"}, \"fill\": {\"color\": \"silver\"}, \"font\": {\"size\": 13}}, \"cells\": {\"values\": [[\"2013-07-01\", \"2013-07-02\", \"2013-07-03\", \"2013-07-05\", \"2013-07-08\", \"2013-07-09\", \"2013-07-10\", \"2013-07-11\", \"2013-07-12\", \"2013-07-15\", \"...\"], [\"16.176\", \"15.820\", \"16.128\", \"16.215\", \"16.311\", \"16.715\", \"16.532\", \"16.725\", \"16.908\", \"17.100\", \"...\"], [\"53.109\", \"54.312\", \"54.612\", \"54.173\", \"53.866\", \"54.813\", \"54.603\", \"55.454\", \"55.353\", \"55.474\", \"...\"], [\"34.924\", \"35.428\", \"35.445\", \"35.856\", \"36.662\", \"36.360\", \"36.855\", \"37.082\", \"38.157\", \"37.793\", \"...\"], [\"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\"]], \"line\": {\"color\": \"silver\"}, \"fill\": {\"color\": [\"silver\", \"white\"]}, \"font\": {\"size\": 13}}}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"displayModeBar\": false})});</script>"},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"7HUOEFfgxS4R","colab_type":"text"},"source":["# Part 1: Smart Beta Portfolio\n","In Part 1 of this project, you'll build a portfolio using dividend yield to choose the portfolio weights. A portfolio such as this could be incorporated into a smart beta ETF.  You'll compare this portfolio to a market cap weighted index to see how well it performs. \n","\n","Note that in practice, you'll probably get the index weights from a data vendor (such as companies that create indices, like MSCI, FTSE, Standard and Poor's), but for this exercise we will simulate a market cap weighted index.\n","\n","## Index Weights\n","The index we'll be using is based on large dollar volume stocks. Implement `generate_dollar_volume_weights` to generate the weights for this index. For each date, generate the weights based on dollar volume traded for that date. For example, assume the following is close prices and volume data:\n","```\n","                 Prices\n","               A         B         ...\n","2013-07-08     2         2         ...\n","2013-07-09     5         6         ...\n","2013-07-10     1         2         ...\n","2013-07-11     6         5         ...\n","...            ...       ...       ...\n","\n","                 Volume\n","               A         B         ...\n","2013-07-08     100       340       ...\n","2013-07-09     240       220       ...\n","2013-07-10     120       500       ...\n","2013-07-11     10        100       ...\n","...            ...       ...       ...\n","```\n","The weights created from the function `generate_dollar_volume_weights` should be the following:\n","```\n","               A         B         ...\n","2013-07-08     0.126..   0.194..   ...\n","2013-07-09     0.759..   0.377..   ...\n","2013-07-10     0.075..   0.285..   ...\n","2013-07-11     0.037..   0.142..   ...\n","...            ...       ...       ...\n","```"]},{"cell_type":"code","metadata":{"id":"vuSyP9HkxS4S","colab_type":"code","colab":{}},"source":["def generate_dollar_volume_weights(close, volume):\n","    \"\"\"\n","    Generate dollar volume weights.\n","\n","    Parameters\n","    ----------\n","    close : DataFrame\n","        Close price for each ticker and date\n","    volume : str\n","        Volume for each ticker and date\n","\n","    Returns\n","    -------\n","    dollar_volume_weights : DataFrame\n","        The dollar volume weights for each ticker and date\n","    \"\"\"\n","    assert close.index.equals(volume.index)\n","    assert close.columns.equals(volume.columns)\n","    \n","    #TODO: Implement function\n","\n","    return None\n","\n","project_tests.test_generate_dollar_volume_weights(generate_dollar_volume_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"25p7vDqyxS4U","colab_type":"text"},"source":["### View Data\n","Let's generate the index weights using `generate_dollar_volume_weights` and view them using a heatmap."]},{"cell_type":"code","metadata":{"id":"02w3nTHHxS4V","colab_type":"code","colab":{}},"source":["index_weights = generate_dollar_volume_weights(close, volume)\n","project_helper.plot_weights(index_weights, 'Index Weights')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lKPr3U6ZxS4a","colab_type":"text"},"source":["## Portfolio Weights\n","Now that we have the index weights, let's choose the portfolio weights based on dividend. You would normally calculate the weights based on trailing dividend yield, but we'll simplify this by just calculating the total dividend yield over time.\n","\n","Implement `calculate_dividend_weights` to return the weights for each stock based on its total dividend yield over time. This is similar to generating the weight for the index, but it's using dividend data instead.\n","For example, assume the following is `dividends` data:\n","```\n","                 Prices\n","               A         B\n","2013-07-08     0         0\n","2013-07-09     0         1\n","2013-07-10     0.5       0\n","2013-07-11     0         0\n","2013-07-12     2         0\n","...            ...       ...\n","```\n","The weights created from the function `calculate_dividend_weights` should be the following:\n","```\n","               A         B\n","2013-07-08     NaN       NaN\n","2013-07-09     0         1\n","2013-07-10     0.333..   0.666..\n","2013-07-11     0.333..   0.666..\n","2013-07-12     0.714..   0.285..\n","...            ...       ...\n","```"]},{"cell_type":"code","metadata":{"id":"fCLPrpmRxS4a","colab_type":"code","colab":{}},"source":["def calculate_dividend_weights(dividends):\n","    \"\"\"\n","    Calculate dividend weights.\n","\n","    Parameters\n","    ----------\n","    dividends : DataFrame\n","        Dividend for each stock and date\n","\n","    Returns\n","    -------\n","    dividend_weights : DataFrame\n","        Weights for each stock and date\n","    \"\"\"\n","    #TODO: Implement function\n","\n","    return None\n","\n","project_tests.test_calculate_dividend_weights(calculate_dividend_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b7U2zp8IxS4d","colab_type":"text"},"source":["### View Data\n","Just like the index weights, let's generate the ETF weights and view them using a heatmap."]},{"cell_type":"code","metadata":{"id":"vtCkEa3dxS4e","colab_type":"code","colab":{}},"source":["etf_weights = calculate_dividend_weights(dividends)\n","project_helper.plot_weights(etf_weights, 'ETF Weights')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pvDjdHzRxS4g","colab_type":"text"},"source":["## Returns\n","Implement `generate_returns` to generate returns data for all the stocks and dates from price data. You might notice we're implementing returns and not log returns. Since we're not dealing with volatility, we don't have to use log returns."]},{"cell_type":"code","metadata":{"id":"n8fulbALxS4g","colab_type":"code","colab":{}},"source":["def generate_returns(prices):\n","    \"\"\"\n","    Generate returns for ticker and date.\n","\n","    Parameters\n","    ----------\n","    prices : DataFrame\n","        Price for each ticker and date\n","\n","    Returns\n","    -------\n","    returns : Dataframe\n","        The returns for each ticker and date\n","    \"\"\"\n","    #TODO: Implement function\n","\n","    return None\n","\n","project_tests.test_generate_returns(generate_returns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AsPC4bV1xS4j","colab_type":"text"},"source":["### View Data\n","Let's generate the closing returns using `generate_returns` and view them using a heatmap."]},{"cell_type":"code","metadata":{"id":"51ySFBFaxS4j","colab_type":"code","colab":{}},"source":["returns = generate_returns(close)\n","project_helper.plot_returns(returns, 'Close Returns')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q9iDPPGRxS4n","colab_type":"text"},"source":["## Weighted Returns\n","With the returns of each stock computed, we can use it to compute the returns for an index or ETF. Implement `generate_weighted_returns` to create weighted returns using the returns and weights."]},{"cell_type":"code","metadata":{"id":"IC9SGtnVxS4n","colab_type":"code","colab":{}},"source":["def generate_weighted_returns(returns, weights):\n","    \"\"\"\n","    Generate weighted returns.\n","\n","    Parameters\n","    ----------\n","    returns : DataFrame\n","        Returns for each ticker and date\n","    weights : DataFrame\n","        Weights for each ticker and date\n","\n","    Returns\n","    -------\n","    weighted_returns : DataFrame\n","        Weighted returns for each ticker and date\n","    \"\"\"\n","    assert returns.index.equals(weights.index)\n","    assert returns.columns.equals(weights.columns)\n","    \n","    #TODO: Implement function\n","\n","    return None\n","\n","project_tests.test_generate_weighted_returns(generate_weighted_returns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tbUc8lzRxS4q","colab_type":"text"},"source":["### View Data\n","Let's generate the ETF and index returns using `generate_weighted_returns` and view them using a heatmap."]},{"cell_type":"code","metadata":{"id":"DuC5ai1VxS4q","colab_type":"code","colab":{}},"source":["index_weighted_returns = generate_weighted_returns(returns, index_weights)\n","etf_weighted_returns = generate_weighted_returns(returns, etf_weights)\n","project_helper.plot_returns(index_weighted_returns, 'Index Returns')\n","project_helper.plot_returns(etf_weighted_returns, 'ETF Returns')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ScuslKXFxS4s","colab_type":"text"},"source":["## Cumulative Returns\n","To compare performance between the ETF and Index, we're going to calculate the tracking error. Before we do that, we first need to calculate the index and ETF comulative returns. Implement `calculate_cumulative_returns` to calculate the cumulative returns over time given the returns."]},{"cell_type":"code","metadata":{"id":"has81xAGxS4t","colab_type":"code","colab":{}},"source":["def calculate_cumulative_returns(returns):\n","    \"\"\"\n","    Calculate cumulative returns.\n","\n","    Parameters\n","    ----------\n","    returns : DataFrame\n","        Returns for each ticker and date\n","\n","    Returns\n","    -------\n","    cumulative_returns : Pandas Series\n","        Cumulative returns for each date\n","    \"\"\"\n","    #TODO: Implement function\n","    \n","    return None\n","\n","project_tests.test_calculate_cumulative_returns(calculate_cumulative_returns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MRPYr8bIxS4w","colab_type":"text"},"source":["### View Data\n","Let's generate the ETF and index cumulative returns using `calculate_cumulative_returns` and compare the two."]},{"cell_type":"code","metadata":{"id":"XyNPmPgqxS4w","colab_type":"code","colab":{}},"source":["index_weighted_cumulative_returns = calculate_cumulative_returns(index_weighted_returns)\n","etf_weighted_cumulative_returns = calculate_cumulative_returns(etf_weighted_returns)\n","project_helper.plot_benchmark_returns(index_weighted_cumulative_returns, etf_weighted_cumulative_returns, 'Smart Beta ETF vs Index')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a4fi3M-ixS4z","colab_type":"text"},"source":["## Tracking Error\n","In order to check the performance of the smart beta portfolio, we can calculate the annualized tracking error against the index. Implement `tracking_error` to return the tracking error between the ETF and benchmark.\n","\n","For reference, we'll be using the following annualized tracking error function:\n","$$ TE = \\sqrt{252} * SampleStdev(r_p - r_b) $$\n","\n","Where $ r_p $ is the portfolio/ETF returns and $ r_b $ is the benchmark returns.\n","\n","_Note: When calculating the sample standard deviation, the delta degrees of freedom is 1, which is the also the default value._"]},{"cell_type":"code","metadata":{"id":"7bXwRRtYxS4z","colab_type":"code","colab":{}},"source":["def tracking_error(benchmark_returns_by_date, etf_returns_by_date):\n","    \"\"\"\n","    Calculate the tracking error.\n","\n","    Parameters\n","    ----------\n","    benchmark_returns_by_date : Pandas Series\n","        The benchmark returns for each date\n","    etf_returns_by_date : Pandas Series\n","        The ETF returns for each date\n","\n","    Returns\n","    -------\n","    tracking_error : float\n","        The tracking error\n","    \"\"\"\n","    assert benchmark_returns_by_date.index.equals(etf_returns_by_date.index)\n","    \n","    #TODO: Implement function\n","\n","    return None\n","\n","project_tests.test_tracking_error(tracking_error)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nlp9BLqWxS43","colab_type":"text"},"source":["### View Data\n","Let's generate the tracking error using `tracking_error`."]},{"cell_type":"code","metadata":{"id":"gi97iLq9xS44","colab_type":"code","colab":{}},"source":["smart_beta_tracking_error = tracking_error(np.sum(index_weighted_returns, 1), np.sum(etf_weighted_returns, 1))\n","print('Smart Beta Tracking Error: {}'.format(smart_beta_tracking_error))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7lOP6HLcxS46","colab_type":"text"},"source":["# Part 2: Portfolio Optimization\n","\n","Now, let's create a second portfolio.  We'll still reuse the market cap weighted index, but this will be independent of the dividend-weighted portfolio that we created in part 1.\n","\n","We want to both minimize the portfolio variance and also want to closely track a market cap weighted index.  In other words, we're trying to minimize the distance between the weights of our portfolio and the weights of the index.\n","\n","$Minimize \\left [ \\sigma^2_p + \\lambda \\sqrt{\\sum_{1}^{m}(weight_i - indexWeight_i)^2} \\right  ]$ where $m$ is the number of stocks in the portfolio, and $\\lambda$ is a scaling factor that you can choose.\n","\n","Why are we doing this? One way that investors evaluate a fund is by how well it tracks its index. The fund is still expected to deviate from the index within a certain range in order to improve fund performance.  A way for a fund to track the performance of its benchmark is by keeping its asset weights similar to the weights of the index.  We’d expect that if the fund has the same stocks as the benchmark, and also the same weights for each stock as the benchmark, the fund would yield about the same returns as the benchmark. By minimizing a linear combination of both the portfolio risk and distance between portfolio and benchmark weights, we attempt to balance the desire to minimize portfolio variance with the goal of tracking the index.\n","\n","\n","## Covariance\n","Implement `get_covariance_returns` to calculate the covariance of the `returns`. We'll use this to calculate the portfolio variance.\n","\n","If we have $m$ stock series, the covariance matrix is an $m \\times m$ matrix containing the covariance between each pair of stocks.  We can use [`Numpy.cov`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html) to get the covariance.  We give it a 2D array in which each row is a stock series, and each column is an observation at the same period of time. For any `NaN` values, you can replace them with zeros using the [`DataFrame.fillna`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html) function.\n","\n","The covariance matrix $\\mathbf{P} = \n","\\begin{bmatrix}\n","\\sigma^2_{1,1} & ... & \\sigma^2_{1,m} \\\\ \n","... & ... & ...\\\\\n","\\sigma_{m,1} & ... & \\sigma^2_{m,m}  \\\\\n","\\end{bmatrix}$"]},{"cell_type":"code","metadata":{"id":"q7DXBeutxS46","colab_type":"code","colab":{}},"source":["def get_covariance_returns(returns):\n","    \"\"\"\n","    Calculate covariance matrices.\n","\n","    Parameters\n","    ----------\n","    returns : DataFrame\n","        Returns for each ticker and date\n","\n","    Returns\n","    -------\n","    returns_covariance  : 2 dimensional Ndarray\n","        The covariance of the returns\n","    \"\"\"\n","    #TODO: Implement function\n","    \n","    return None\n","\n","project_tests.test_get_covariance_returns(get_covariance_returns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YVReJei4xS4-","colab_type":"text"},"source":["### View Data\n","Let's look at the covariance generated from `get_covariance_returns`."]},{"cell_type":"code","metadata":{"id":"08uUUqunxS4_","colab_type":"code","colab":{}},"source":["covariance_returns = get_covariance_returns(returns)\n","covariance_returns = pd.DataFrame(covariance_returns, returns.columns, returns.columns)\n","\n","covariance_returns_correlation = np.linalg.inv(np.diag(np.sqrt(np.diag(covariance_returns))))\n","covariance_returns_correlation = pd.DataFrame(\n","    covariance_returns_correlation.dot(covariance_returns).dot(covariance_returns_correlation),\n","    covariance_returns.index,\n","    covariance_returns.columns)\n","\n","project_helper.plot_covariance_returns_correlation(\n","    covariance_returns_correlation,\n","    'Covariance Returns Correlation Matrix')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SfqsNTxXxS5B","colab_type":"text"},"source":["### portfolio variance\n","We can write the portfolio variance $\\sigma^2_p = \\mathbf{x^T} \\mathbf{P} \\mathbf{x}$\n","\n","Recall that the $\\mathbf{x^T} \\mathbf{P} \\mathbf{x}$ is called the quadratic form.\n","We can use the cvxpy function `quad_form(x,P)` to get the quadratic form.\n","\n","### Distance from index weights\n","We want portfolio weights that track the index closely.  So we want to minimize the distance between them.\n","Recall from the Pythagorean theorem that you can get the distance between two points in an x,y plane by adding the square of the x and y distances and taking the square root.  Extending this to any number of dimensions is called the L2 norm.  So: $\\sqrt{\\sum_{1}^{n}(weight_i - indexWeight_i)^2}$  Can also be written as $\\left \\| \\mathbf{x} - \\mathbf{index} \\right \\|_2$.  There's a cvxpy function called [norm()](https://www.cvxpy.org/api_reference/cvxpy.atoms.other_atoms.html#norm)\n","`norm(x, p=2, axis=None)`.  The default is already set to find an L2 norm, so you would pass in one argument, which is the difference between your portfolio weights and the index weights.\n","\n","### objective function\n","We want to minimize both the portfolio variance and the distance of the portfolio weights from the index weights.\n","We also want to choose a `scale` constant, which is $\\lambda$ in the expression. \n","\n","$\\mathbf{x^T} \\mathbf{P} \\mathbf{x} + \\lambda \\left \\| \\mathbf{x} - \\mathbf{index} \\right \\|_2$\n","\n","\n","This lets us choose how much priority we give to minimizing the difference from the index, relative to minimizing the variance of the portfolio.  If you choose a higher value for `scale` ($\\lambda$).\n","\n","We can find the objective function using cvxpy `objective = cvx.Minimize()`.  Can you guess what to pass into this function?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MMawg3TIxS5C","colab_type":"text"},"source":["### constraints\n","We can also define our constraints in a list.  For example, you'd want the weights to sum to one. So $\\sum_{1}^{n}x = 1$.  You may also need to go long only, which means no shorting, so no negative weights.  So $x_i >0 $ for all $i$. you could save a variable as `[x >= 0, sum(x) == 1]`, where x was created using `cvx.Variable()`.\n","\n","### optimization\n","So now that we have our objective function and constraints, we can solve for the values of $\\mathbf{x}$.\n","cvxpy has the constructor `Problem(objective, constraints)`, which returns a `Problem` object.\n","\n","The `Problem` object has a function solve(), which returns the minimum of the solution.  In this case, this is the minimum variance of the portfolio.\n","\n","It also updates the vector $\\mathbf{x}$.\n","\n","We can check out the values of $x_A$ and $x_B$ that gave the minimum portfolio variance by using `x.value`"]},{"cell_type":"code","metadata":{"id":"sMWIGtZrxS5C","colab_type":"code","colab":{}},"source":["import cvxpy as cvx\n","\n","def get_optimal_weights(covariance_returns, index_weights, scale=2.0):\n","    \"\"\"\n","    Find the optimal weights.\n","\n","    Parameters\n","    ----------\n","    covariance_returns : 2 dimensional Ndarray\n","        The covariance of the returns\n","    index_weights : Pandas Series\n","        Index weights for all tickers at a period in time\n","    scale : int\n","        The penalty factor for weights the deviate from the index \n","    Returns\n","    -------\n","    x : 1 dimensional Ndarray\n","        The solution for x\n","    \"\"\"\n","    assert len(covariance_returns.shape) == 2\n","    assert len(index_weights.shape) == 1\n","    assert covariance_returns.shape[0] == covariance_returns.shape[1]  == index_weights.shape[0]\n","\n","    #TODO: Implement function\n","    \n","    return None\n","\n","project_tests.test_get_optimal_weights(get_optimal_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YoHhkNX9xS5F","colab_type":"text"},"source":["## Optimized Portfolio\n","Using the `get_optimal_weights` function, let's generate the optimal ETF weights without rebalanceing. We can do this by feeding in the covariance of the entire history of data. We also need to feed in a set of index weights. We'll go with the average weights of the index over time."]},{"cell_type":"code","metadata":{"id":"iQmB5T88xS5F","colab_type":"code","colab":{}},"source":["raw_optimal_single_rebalance_etf_weights = get_optimal_weights(covariance_returns.values, index_weights.iloc[-1])\n","optimal_single_rebalance_etf_weights = pd.DataFrame(\n","    np.tile(raw_optimal_single_rebalance_etf_weights, (len(returns.index), 1)),\n","    returns.index,\n","    returns.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7qekcfMsxS5I","colab_type":"text"},"source":["With our ETF weights built, let's compare it to the index. Run the next cell to calculate the ETF returns and compare it to the index returns."]},{"cell_type":"code","metadata":{"id":"MqzW-gwTxS5I","colab_type":"code","colab":{}},"source":["optim_etf_returns = generate_weighted_returns(returns, optimal_single_rebalance_etf_weights)\n","optim_etf_cumulative_returns = calculate_cumulative_returns(optim_etf_returns)\n","project_helper.plot_benchmark_returns(index_weighted_cumulative_returns, optim_etf_cumulative_returns, 'Optimized ETF vs Index')\n","\n","optim_etf_tracking_error = tracking_error(np.sum(index_weighted_returns, 1), np.sum(optim_etf_returns, 1))\n","print('Optimized ETF Tracking Error: {}'.format(optim_etf_tracking_error))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wzv21u-txS5M","colab_type":"text"},"source":["## Rebalance Portfolio Over Time\n","The single optimized ETF portfolio used the same weights for the entire history. This might not be the optimal weights for the entire period. Let's rebalance the portfolio over the same period instead of using the same weights. Implement `rebalance_portfolio` to rebalance a portfolio.\n","\n","Reblance the portfolio every n number of days, which is given as `shift_size`. When rebalancing, you should look back a certain number of days of data in the past, denoted as `chunk_size`. Using this data, compute the optoimal weights using `get_optimal_weights` and `get_covariance_returns`."]},{"cell_type":"code","metadata":{"id":"3q4xjEU4xS5M","colab_type":"code","colab":{}},"source":["def rebalance_portfolio(returns, index_weights, shift_size, chunk_size):\n","    \"\"\"\n","    Get weights for each rebalancing of the portfolio.\n","\n","    Parameters\n","    ----------\n","    returns : DataFrame\n","        Returns for each ticker and date\n","    index_weights : DataFrame\n","        Index weight for each ticker and date\n","    shift_size : int\n","        The number of days between each rebalance\n","    chunk_size : int\n","        The number of days to look in the past for rebalancing\n","\n","    Returns\n","    -------\n","    all_rebalance_weights  : list of Ndarrays\n","        The ETF weights for each point they are rebalanced\n","    \"\"\"\n","    assert returns.index.equals(index_weights.index)\n","    assert returns.columns.equals(index_weights.columns)\n","    assert shift_size > 0\n","    assert chunk_size >= 0\n","    \n","    #TODO: Implement function\n","    \n","    return None\n","\n","project_tests.test_rebalance_portfolio(rebalance_portfolio)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Y-zNXm4xS5Q","colab_type":"text"},"source":["Run the following cell to rebalance the portfolio using `rebalance_portfolio`."]},{"cell_type":"code","metadata":{"id":"Veb_92X2xS5Q","colab_type":"code","colab":{}},"source":["chunk_size = 250\n","shift_size = 5\n","all_rebalance_weights = rebalance_portfolio(returns, index_weights, shift_size, chunk_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0vgMFGLBxS5S","colab_type":"text"},"source":["## Portfolio Turnover\n","With the portfolio rebalanced, we need to use a metric to measure the cost of rebalancing the portfolio. Implement `get_portfolio_turnover` to calculate the annual portfolio turnover. We'll be using the formulas used in the classroom:\n","\n","$ AnnualizedTurnover =\\frac{SumTotalTurnover}{NumberOfRebalanceEvents} * NumberofRebalanceEventsPerYear $\n","\n","$ SumTotalTurnover =\\sum_{t,n}{\\left | x_{t,n} - x_{t+1,n} \\right |} $ Where $ x_{t,n} $ are the weights at time $ t $ for equity $ n $.\n","\n","$ SumTotalTurnover $ is just a different way of writing $ \\sum \\left | x_{t_1,n} - x_{t_2,n} \\right | $"]},{"cell_type":"code","metadata":{"id":"xVyTiV3TxS5S","colab_type":"code","colab":{}},"source":["def get_portfolio_turnover(all_rebalance_weights, shift_size, rebalance_count, n_trading_days_in_year=252):\n","    \"\"\"\n","    Calculage portfolio turnover.\n","\n","    Parameters\n","    ----------\n","    all_rebalance_weights : list of Ndarrays\n","        The ETF weights for each point they are rebalanced\n","    shift_size : int\n","        The number of days between each rebalance\n","    rebalance_count : int\n","        Number of times the portfolio was rebalanced\n","    n_trading_days_in_year: int\n","        Number of trading days in a year\n","\n","    Returns\n","    -------\n","    portfolio_turnover  : float\n","        The portfolio turnover\n","    \"\"\"\n","    assert shift_size > 0\n","    assert rebalance_count > 0\n","    \n","    #TODO: Implement function\n","    \n","    return None\n","\n","project_tests.test_get_portfolio_turnover(get_portfolio_turnover)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qFiWgYDkxS5U","colab_type":"text"},"source":["Run the following cell to get the portfolio turnover from  `get_portfolio turnover`."]},{"cell_type":"code","metadata":{"id":"FZZCIEYhxS5V","colab_type":"code","colab":{}},"source":["print(get_portfolio_turnover(all_rebalance_weights, shift_size, len(all_rebalance_weights) - 1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-3w2KNVjxS5X","colab_type":"text"},"source":["That's it! You've built a smart beta portfolio in part 1 and did portfolio optimization in part 2. You can now submit your project."]},{"cell_type":"markdown","metadata":{"id":"7pWYhIxgxS5X","colab_type":"text"},"source":["## Submission\n","Now that you're done with the project, it's time to submit it. Click the submit button in the bottom right. One of our reviewers will give you feedback on your project with a pass or not passed grade. You can continue to the next section while you wait for feedback."]}]}